{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "EOm6Cai_2zA9",
      "metadata": {
        "id": "EOm6Cai_2zA9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Training Code\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9511456a-5878-4bd4-a985-2c1dfc0f255d",
      "metadata": {
        "id": "9511456a-5878-4bd4-a985-2c1dfc0f255d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "system_directory = \"/content/drive/MyDrive/Upwork/CellularImage-Unet/CodeSpace\"\n",
        "sys.path.append(system_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5LedXGCpDlF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5LedXGCpDlF",
        "outputId": "99780a4a-7872-4bcb-f608-d36ebcf7401b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "IVGSnO6GZpJI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVGSnO6GZpJI",
        "outputId": "b52558f2-3f2b-42ce-aa67-32a398cddfeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib tqdm numpy torchsummary torchinfo einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6774882a-53a2-4525-9ffc-a61a4c08b2ab",
      "metadata": {
        "id": "6774882a-53a2-4525-9ffc-a61a4c08b2ab"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "# from diffusion_utilities import *\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "from torch import einsum\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "from torch import einsum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7904a769-0391-4ed7-95c9-f7c9f210edfb",
      "metadata": {
        "id": "7904a769-0391-4ed7-95c9-f7c9f210edfb"
      },
      "outputs": [],
      "source": [
        "# Necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from torch import einsum\n",
        "import math\n",
        "\n",
        "\"\"\"\n",
        "Positional Encoding\n",
        "\"\"\"\n",
        "class PositionalEncoding2D(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(PositionalEncoding2D, self).__init__()\n",
        "        if channels % 4 != 0:\n",
        "            raise ValueError(\"Channels must be divisible by 4\")\n",
        "        self.channels = channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        device = x.device\n",
        "\n",
        "        num_pos_feats = channels // 2\n",
        "\n",
        "        # Create grid of positions\n",
        "        y_embed = torch.linspace(-1, 1, height, device=device).unsqueeze(1).repeat(1, width)  # (height, width)\n",
        "        x_embed = torch.linspace(-1, 1, width, device=device).unsqueeze(0).repeat(height, 1)  # (height, width)\n",
        "\n",
        "        # Calculate div_term\n",
        "        dim_t = torch.arange(num_pos_feats // 2, dtype=torch.float32, device=device)\n",
        "        dim_t = 10000 ** (2 * (dim_t // 2) / num_pos_feats)\n",
        "\n",
        "        # Compute positional embeddings\n",
        "        pos_x = x_embed.unsqueeze(-1) / dim_t\n",
        "        pos_y = y_embed.unsqueeze(-1) / dim_t\n",
        "\n",
        "        # Interleave sine and cosine for x and y\n",
        "        pos_x = torch.stack((pos_x.sin(), pos_x.cos()), dim=-1).view(height, width, -1)\n",
        "        pos_y = torch.stack((pos_y.sin(), pos_y.cos()), dim=-1).view(height, width, -1)\n",
        "\n",
        "        # Concatenate x and y embeddings\n",
        "        pos_emb = torch.cat((pos_y, pos_x), dim=-1)  # Shape: (height, width, channels)\n",
        "\n",
        "        # Permute to match the shape (batch_size, channels, height, width)\n",
        "        pos_emb = pos_emb.permute(2, 0, 1).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "        return pos_emb\n",
        "\n",
        "\"\"\"\n",
        "Model Utilities with Enhancements\n",
        "\"\"\"\n",
        "def get_num_groups(num_channels):\n",
        "    for num_groups in [32, 16, 8, 4, 2, 1]:\n",
        "        if num_channels % num_groups == 0:\n",
        "            return num_groups\n",
        "    return 1\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, is_res=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.same_channels = in_channels == out_channels\n",
        "        self.is_res = is_res\n",
        "\n",
        "        num_groups_in = get_num_groups(in_channels)\n",
        "        num_groups_out = get_num_groups(out_channels)\n",
        "\n",
        "        # Pre-activation residual block with GroupNorm\n",
        "        self.norm1 = nn.GroupNorm(num_groups_in, in_channels)\n",
        "        self.act1 = nn.GELU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(num_groups_out, out_channels)\n",
        "        self.act2 = nn.GELU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "\n",
        "        if not self.same_channels:\n",
        "            self.conv1x1 = nn.Conv2d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.norm1(x)\n",
        "        out = self.act1(out)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.norm2(out)\n",
        "        out = self.act2(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        if self.is_res:\n",
        "            if not self.same_channels:\n",
        "                identity = self.conv1x1(identity)\n",
        "            out += identity\n",
        "            out = out / 1.414\n",
        "\n",
        "        return out\n",
        "\n",
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetDown, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            ResidualConvBlock(in_channels, out_channels),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "        )\n",
        "        self.down = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x_down = self.down(x)\n",
        "        return x_down, x  # Return both for skip connections\n",
        "\n",
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels):\n",
        "        super(UnetUp, self).__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(get_num_groups(out_channels), out_channels),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            ResidualConvBlock(out_channels + skip_channels, out_channels),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        # Ensure spatial dimensions match\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[-2:], mode='nearest')\n",
        "        x = torch.cat((x, skip), dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedFC, self).__init__()\n",
        "        '''\n",
        "        This class defines a generic two-layer feed-forward neural network for embedding input data of\n",
        "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
        "        '''\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Define the layers for the network\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        # Apply the model layers to the flattened tensor\n",
        "        return self.model(x)\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size=(16, 16), heads=8, shifted=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.window_size = window_size  # For relative positional encoding\n",
        "        self.head_dim = dim // heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.shifted = shifted  # Whether to apply shifted window\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "        # Relative positional bias table\n",
        "        num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1)\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros(num_relative_distance, heads)\n",
        "        )\n",
        "\n",
        "        # Get pairwise relative position indices for each token in the window\n",
        "        coords_h = torch.arange(window_size[0])\n",
        "        coords_w = torch.arange(window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid(coords_h, coords_w))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += window_size[0] - 1  # Shift to start from 0\n",
        "        relative_coords[:, :, 1] += window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W)\n",
        "        mask: (num_windows, num_windows) or None\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        window_h, window_w = self.window_size\n",
        "        shifted = self.shifted\n",
        "\n",
        "        if shifted:\n",
        "            # Calculate shift size (usually half the window size)\n",
        "            shift_size_h = window_h // 2\n",
        "            shift_size_w = window_w // 2\n",
        "            # Shift the feature map\n",
        "            x = torch.roll(x, shifts=(-shift_size_h, -shift_size_w), dims=(2, 3))\n",
        "        else:\n",
        "            shift_size_h = 0\n",
        "            shift_size_w = 0\n",
        "\n",
        "        # Pad feature maps to multiples of window size\n",
        "        pad_h = (window_h - H % window_h) if H % window_h != 0 else 0\n",
        "        pad_w = (window_w - W % window_w) if W % window_w != 0 else 0\n",
        "        x = F.pad(x, (0, pad_w, 0, pad_h))  # Pad last two dimensions\n",
        "        B, C, H_padded, W_padded = x.shape\n",
        "\n",
        "        # Create attention mask if shifted\n",
        "        if shifted:\n",
        "            # Calculate number of patches\n",
        "            img_mask = torch.zeros((1, H_padded, W_padded, 1), device=x.device)\n",
        "            # Compute the blocks\n",
        "            h_blocks = H_padded // window_h\n",
        "            w_blocks = W_padded // window_w\n",
        "\n",
        "            # Create mask for shifted windows\n",
        "            cnt = 0\n",
        "            for h in range(h_blocks):\n",
        "                for w in range(w_blocks):\n",
        "                    img_mask[:, h * window_h:(h + 1) * window_h, w * window_w:(w + 1) * window_w, :] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "            mask_windows = self.window_partition(img_mask.permute(0, 3, 1, 2), window_h, window_w)  # (num_windows*B, window_h*window_w, 1)\n",
        "            mask_windows = mask_windows.squeeze(-1)  # (num_windows*B, window_h*window_w)\n",
        "\n",
        "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        # Partition windows\n",
        "        x_windows = self.window_partition(x, window_h, window_w)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Compute QKV\n",
        "        qkv = self.to_qkv(x_windows).reshape(x_windows.shape[0], x_windows.shape[1], 3, self.heads, self.head_dim)\n",
        "        q, k, v = qkv.permute(2, 0, 3, 1, 4).unbind(0)  # Each has shape (num_windows*B, heads, window_size*window_size, head_dim)\n",
        "\n",
        "        # Compute attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (num_windows*B, heads, window_size*window_size, window_size*window_size)\n",
        "\n",
        "        # Add relative positional bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            window_h * window_w, window_h * window_w, -1\n",
        "        )  # (window_size*window_size, window_size*window_size, heads)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # (heads, window_size*window_size, window_size*window_size)\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if shifted:\n",
        "            # Apply attention mask\n",
        "            attn = attn.view(B, -1, self.heads, window_h * window_w, window_h * window_w)\n",
        "            attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.heads, window_h * window_w, window_h * window_w)\n",
        "            attn = attn.softmax(dim=-1)\n",
        "        else:\n",
        "            attn = attn.softmax(dim=-1)\n",
        "\n",
        "        # Apply attention to V\n",
        "        out = (attn @ v).transpose(1, 2).reshape(x_windows.shape[0], window_h * window_w, C)\n",
        "\n",
        "        # Project out\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        # Reverse window partitioning\n",
        "        out = out.view(-1, window_h, window_w, C)\n",
        "        x = self.window_reverse(out, H_padded, W_padded, window_h, window_w)  # (B, C, H_padded, W_padded)\n",
        "\n",
        "        if shifted:\n",
        "            # Reverse the shift\n",
        "            x = torch.roll(x, shifts=(shift_size_h, shift_size_w), dims=(2, 3))\n",
        "\n",
        "        # Remove padding\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = x[:, :, :H, :W]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def window_partition(self, x, window_h, window_w):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, C, H, W)\n",
        "            window_h, window_w: window size\n",
        "        Returns:\n",
        "            windows: (num_windows*B, window_size*window_size, C)\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, H // window_h, window_h, W // window_w, window_w)\n",
        "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous().view(-1, window_h * window_w, C)\n",
        "        return x\n",
        "\n",
        "    def window_reverse(self, windows, H, W, window_h, window_w):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            windows: (num_windows*B, window_size*window_size, C)\n",
        "            H, W: Height and Width of image\n",
        "            window_h, window_w: window size\n",
        "        Returns:\n",
        "            x: (B, C, H, W)\n",
        "        \"\"\"\n",
        "        B = int(windows.shape[0] / (H * W / (window_h * window_w)))\n",
        "        x = windows.view(B, H // window_h, W // window_w, window_h, window_w, -1)\n",
        "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous().view(B, -1, H, W)\n",
        "        return x\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, dim, context_dim, heads=1, dim_head=None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.context_dim = context_dim\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head or (dim // heads)\n",
        "        inner_dim = self.dim_head * heads\n",
        "        self.scale = self.dim_head ** -0.5\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        x = x.view(b, c, -1).permute(0, 2, 1)  # Shape: [b, hw, c]\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(context).unsqueeze(1)     # Shape: [b, 1, inner_dim]\n",
        "        v = self.to_v(context).unsqueeze(1)     # Shape: [b, 1, inner_dim]\n",
        "\n",
        "        q = q.view(b, -1, self.heads, self.dim_head).transpose(1, 2)  # [b, heads, hw, dim_head]\n",
        "        k = k.view(b, -1, self.heads, self.dim_head).transpose(1, 2)  # [b, heads, 1, dim_head]\n",
        "        v = v.view(b, -1, self.heads, self.dim_head).transpose(1, 2)  # [b, heads, 1, dim_head]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [b, heads, hw, 1]\n",
        "        attn = attn.softmax(dim=-2)  # Softmax over the query dimension\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(b, -1, self.dim)  # [b, hw, dim]\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        out = out.permute(0, 2, 1).view(b, c, h, w)\n",
        "        return out\n",
        "\n",
        "class ContextUnet(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat=64, n_cfeat=1536):\n",
        "        super(ContextUnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "        self.n_cfeat = n_cfeat\n",
        "\n",
        "        # Encoder\n",
        "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
        "        self.down1 = UnetDown(n_feat, 2 * n_feat)\n",
        "        self.down2 = UnetDown(2 * n_feat, 4 * n_feat)\n",
        "        self.down3 = UnetDown(4 * n_feat, 8 * n_feat)\n",
        "\n",
        "        self.to_vec = nn.Identity()  # No spatial dimension reduction\n",
        "\n",
        "        # Embeddings\n",
        "        self.timeembed1 = EmbedFC(1, 8 * n_feat)\n",
        "        self.timeembed2 = EmbedFC(1, 4 * n_feat)\n",
        "        self.timeembed3 = EmbedFC(1, 2 * n_feat)\n",
        "\n",
        "        self.contextembed1 = EmbedFC(n_cfeat, 8 * n_feat)\n",
        "        self.contextembed2 = EmbedFC(n_cfeat, 4 * n_feat)\n",
        "        self.contextembed3 = EmbedFC(n_cfeat, 2 * n_feat)\n",
        "\n",
        "        # Decoder\n",
        "        self.up0 = nn.Sequential(\n",
        "            ResidualConvBlock(8 * n_feat, 8 * n_feat),\n",
        "            ResidualConvBlock(8 * n_feat, 8 * n_feat),\n",
        "        )\n",
        "        self.up1 = UnetUp(in_channels=8 * n_feat, skip_channels=8 * n_feat, out_channels=4 * n_feat)\n",
        "        self.up2 = UnetUp(in_channels=4 * n_feat, skip_channels=4 * n_feat, out_channels=2 * n_feat)\n",
        "        self.up3 = UnetUp(in_channels=2 * n_feat, skip_channels=2 * n_feat, out_channels=n_feat)\n",
        "\n",
        "        self.out = nn.Conv2d(n_feat, self.in_channels, 3, 1, 1)\n",
        "\n",
        "        # Initialize attention layers with different window sizes and shifted flags\n",
        "        self.self_attn1 = MultiHeadSelfAttention(dim=8 * n_feat, window_size=(8, 8), heads=8, shifted=False)\n",
        "        self.self_attn2 = MultiHeadSelfAttention(dim=4 * n_feat, window_size=(4, 4), heads=8, shifted=True)\n",
        "        self.self_attn3 = MultiHeadSelfAttention(dim=2 * n_feat, window_size=(2, 2), heads=8, shifted=False)\n",
        "\n",
        "        self.cross_attn1 = MultiHeadCrossAttention(dim=8 * n_feat, context_dim=8 * n_feat, heads=1)\n",
        "        self.cross_attn2 = MultiHeadCrossAttention(dim=4 * n_feat, context_dim=4 * n_feat, heads=1)\n",
        "        self.cross_attn3 = MultiHeadCrossAttention(dim=2 * n_feat, context_dim=2 * n_feat, heads=1)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pos_encoding = PositionalEncoding2D(n_feat)\n",
        "\n",
        "        # Track shift state for alternating layers\n",
        "        self.shift = False\n",
        "\n",
        "    def forward(self, x, t, c=None):\n",
        "        # Encoder\n",
        "        x = self.init_conv(x)\n",
        "        x = x + self.pos_encoding(x)  # Apply positional encoding\n",
        "        down1, skip1 = self.down1(x)\n",
        "        down2, skip2 = self.down2(down1)\n",
        "        down3, skip3 = self.down3(down2)\n",
        "\n",
        "        hiddenvec = self.to_vec(down3)\n",
        "\n",
        "        if c is None:\n",
        "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x.device)\n",
        "\n",
        "        c_embed1 = self.contextembed1(c).unsqueeze(-1).unsqueeze(-1)\n",
        "        c_embed2 = self.contextembed2(c).unsqueeze(-1).unsqueeze(-1)\n",
        "        c_embed3 = self.contextembed3(c).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        temb1 = self.timeembed1(t).unsqueeze(-1).unsqueeze(-1)\n",
        "        temb2 = self.timeembed2(t).unsqueeze(-1).unsqueeze(-1)\n",
        "        temb3 = self.timeembed3(t).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Decoder\n",
        "        up1 = self.up0(hiddenvec)\n",
        "\n",
        "        # Apply self-attention with window size (4x4)\n",
        "        up1 = self.self_attn1(up1)\n",
        "        up1 = self.cross_attn1(up1, c_embed1.squeeze(-1).squeeze(-1))\n",
        "        up1 = up1 + temb1\n",
        "\n",
        "        up1 = self.up1(up1, skip3)\n",
        "\n",
        "        # Apply self-attention with window size (2x2)\n",
        "        up2 = self.self_attn2(up1)\n",
        "        up2 = self.cross_attn2(up2, c_embed2.squeeze(-1).squeeze(-1))\n",
        "        up2 = up2 + temb2\n",
        "\n",
        "        up2 = self.up2(up2, skip2)\n",
        "\n",
        "        # Apply self-attention with window size (1x1)\n",
        "        up3 = self.self_attn3(up2)\n",
        "        up3 = self.cross_attn3(up3, c_embed3.squeeze(-1).squeeze(-1))\n",
        "        up3 = up3 + temb3\n",
        "\n",
        "        up3 = self.up3(up3, skip1)\n",
        "\n",
        "        out = self.out(up3)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0e104b67-9344-4110-baf4-2b25dcc457f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e104b67-9344-4110-baf4-2b25dcc457f4",
        "outputId": "7c99be6e-d1cf-42d8-f867-6c1abac6f81b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "\n",
        "# diffusion hyperparameters\n",
        "timesteps = 1000\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.02\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# network hyperparameters\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
        "n_feat = 128 # 128 hidden dimension feature\n",
        "n_cfeat = 1536 # context vector is of size 1536\n",
        "height = 128 # 128x128 image\n",
        "save_dir = f\"{system_directory}/Weights/\"\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 50\n",
        "n_epoch = 200\n",
        "lrate = 1e-5\n",
        "\n",
        "\n",
        "\n",
        "n_gpus = torch.cuda.device_count()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat).to(device)\n",
        "if n_gpus > 1:\n",
        "    nn_model = nn.DataParallel(nn_model)\n",
        "\n",
        "nn_model = nn_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "af4eeea2-aefa-49e2-a3fc-7ed88c3659e7",
      "metadata": {
        "id": "af4eeea2-aefa-49e2-a3fc-7ed88c3659e7"
      },
      "outputs": [],
      "source": [
        "# construct DDPM noise schedule\n",
        "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
        "a_t = 1 - b_t\n",
        "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
        "ab_t[0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e0a2f2c7-d7ae-41b2-9a31-62e7380c2336",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0a2f2c7-d7ae-41b2-9a31-62e7380c2336",
        "outputId": "078a437b-a454-4b03-9c31-bf66fddc0a53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "ModelWrapper                                  [1, 3, 128, 128]          --\n",
              "├─ContextUnet: 1-1                            [1, 3, 128, 128]          --\n",
              "│    └─ResidualConvBlock: 2-1                 [1, 128, 128, 128]        --\n",
              "│    │    └─GroupNorm: 3-1                    [1, 3, 128, 128]          6\n",
              "│    │    └─GELU: 3-2                         [1, 3, 128, 128]          --\n",
              "│    │    └─Conv2d: 3-3                       [1, 128, 128, 128]        3,584\n",
              "│    │    └─GroupNorm: 3-4                    [1, 128, 128, 128]        256\n",
              "│    │    └─GELU: 3-5                         [1, 128, 128, 128]        --\n",
              "│    │    └─Conv2d: 3-6                       [1, 128, 128, 128]        147,584\n",
              "│    │    └─Conv2d: 3-7                       [1, 128, 128, 128]        512\n",
              "│    └─PositionalEncoding2D: 2-2              [1, 128, 128, 128]        --\n",
              "│    └─UnetDown: 2-3                          [1, 256, 64, 64]          --\n",
              "│    │    └─Sequential: 3-8                   [1, 256, 128, 128]        2,100,224\n",
              "│    │    └─MaxPool2d: 3-9                    [1, 256, 64, 64]          --\n",
              "│    └─UnetDown: 2-4                          [1, 512, 32, 32]          --\n",
              "│    │    └─Sequential: 3-10                  [1, 512, 64, 64]          8,394,752\n",
              "│    │    └─MaxPool2d: 3-11                   [1, 512, 32, 32]          --\n",
              "│    └─UnetDown: 2-5                          [1, 1024, 16, 16]         --\n",
              "│    │    └─Sequential: 3-12                  [1, 1024, 32, 32]         33,566,720\n",
              "│    │    └─MaxPool2d: 3-13                   [1, 1024, 16, 16]         --\n",
              "│    └─Identity: 2-6                          [1, 1024, 16, 16]         --\n",
              "│    └─EmbedFC: 2-7                           [1, 1024]                 --\n",
              "│    │    └─Sequential: 3-14                  [1, 1024]                 2,623,488\n",
              "│    └─EmbedFC: 2-8                           [1, 512]                  --\n",
              "│    │    └─Sequential: 3-15                  [1, 512]                  1,049,600\n",
              "│    └─EmbedFC: 2-9                           [1, 256]                  --\n",
              "│    │    └─Sequential: 3-16                  [1, 256]                  459,264\n",
              "│    └─EmbedFC: 2-10                          [1, 1024]                 --\n",
              "│    │    └─Sequential: 3-17                  [1, 1024]                 1,051,648\n",
              "│    └─EmbedFC: 2-11                          [1, 512]                  --\n",
              "│    │    └─Sequential: 3-18                  [1, 512]                  263,680\n",
              "│    └─EmbedFC: 2-12                          [1, 256]                  --\n",
              "│    │    └─Sequential: 3-19                  [1, 256]                  66,304\n",
              "│    └─Sequential: 2-13                       [1, 1024, 16, 16]         --\n",
              "│    │    └─ResidualConvBlock: 3-20           [1, 1024, 16, 16]         18,880,512\n",
              "│    │    └─ResidualConvBlock: 3-21           [1, 1024, 16, 16]         18,880,512\n",
              "│    └─MultiHeadSelfAttention: 2-14           [1, 1024, 16, 16]         1,800\n",
              "│    │    └─Linear: 3-22                      [4, 64, 3072]             3,145,728\n",
              "│    │    └─Linear: 3-23                      [4, 64, 1024]             1,049,600\n",
              "│    └─MultiHeadCrossAttention: 2-15          [1, 1024, 16, 16]         --\n",
              "│    │    └─Linear: 3-24                      [1, 256, 1024]            1,048,576\n",
              "│    │    └─Linear: 3-25                      [1, 1024]                 1,048,576\n",
              "│    │    └─Linear: 3-26                      [1, 1024]                 1,048,576\n",
              "│    │    └─Linear: 3-27                      [1, 256, 1024]            1,049,600\n",
              "│    └─UnetUp: 2-16                           [1, 512, 32, 32]          --\n",
              "│    │    └─Sequential: 3-28                  [1, 512, 32, 32]          4,720,128\n",
              "│    │    └─Sequential: 3-29                  [1, 512, 32, 32]          14,950,912\n",
              "│    └─MultiHeadSelfAttention: 2-17           [1, 512, 32, 32]          392\n",
              "│    │    └─Linear: 3-30                      [64, 16, 1536]            786,432\n",
              "│    │    └─Linear: 3-31                      [64, 16, 512]             262,656\n",
              "│    └─MultiHeadCrossAttention: 2-18          [1, 512, 32, 32]          --\n",
              "│    │    └─Linear: 3-32                      [1, 1024, 512]            262,144\n",
              "│    │    └─Linear: 3-33                      [1, 512]                  262,144\n",
              "│    │    └─Linear: 3-34                      [1, 512]                  262,144\n",
              "│    │    └─Linear: 3-35                      [1, 1024, 512]            262,656\n",
              "│    └─UnetUp: 2-19                           [1, 256, 64, 64]          --\n",
              "│    │    └─Sequential: 3-36                  [1, 256, 64, 64]          1,180,416\n",
              "│    │    └─Sequential: 3-37                  [1, 256, 64, 64]          3,739,904\n",
              "│    └─MultiHeadSelfAttention: 2-20           [1, 256, 64, 64]          72\n",
              "│    │    └─Linear: 3-38                      [1024, 4, 768]            196,608\n",
              "│    │    └─Linear: 3-39                      [1024, 4, 256]            65,792\n",
              "│    └─MultiHeadCrossAttention: 2-21          [1, 256, 64, 64]          --\n",
              "│    │    └─Linear: 3-40                      [1, 4096, 256]            65,536\n",
              "│    │    └─Linear: 3-41                      [1, 256]                  65,536\n",
              "│    │    └─Linear: 3-42                      [1, 256]                  65,536\n",
              "│    │    └─Linear: 3-43                      [1, 4096, 256]            65,792\n",
              "│    └─UnetUp: 2-22                           [1, 128, 128, 128]        --\n",
              "│    │    └─Sequential: 3-44                  [1, 128, 128, 128]        295,296\n",
              "│    │    └─Sequential: 3-45                  [1, 128, 128, 128]        936,064\n",
              "│    └─Conv2d: 2-23                           [1, 3, 128, 128]          3,459\n",
              "===============================================================================================\n",
              "Total params: 124,330,721\n",
              "Trainable params: 124,330,721\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 176.11\n",
              "===============================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 1053.64\n",
              "Params size (MB): 497.31\n",
              "Estimated Total Size (MB): 1551.15\n",
              "==============================================================================================="
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Wrap the model\n",
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWrapper, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3:\n",
        "            x = x.unsqueeze(0)  # Add batch dimension if missing\n",
        "        t = torch.zeros((x.size(0), 1), device=x.device)  # Dummy timestep\n",
        "        c = torch.zeros((x.size(0), n_cfeat), device=x.device)  # Dummy context\n",
        "        return self.model(x, t, c)\n",
        "# nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat).to(device)\n",
        "\n",
        "model_wrapper = ModelWrapper(nn_model)\n",
        "\n",
        "# Get the model summary\n",
        "summary(model_wrapper, input_size=(1, 3, 128, 128))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffcd447-5f79-40dc-90b8-2f469ee08068",
      "metadata": {
        "id": "8ffcd447-5f79-40dc-90b8-2f469ee08068"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                # from [0,255] to range [0.0,1.0]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # range [-1,1]\n",
        "])\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.images = np.array([row[1] for row in data])[:, :, :, :3]\n",
        "        self.embeddings = np.array([row[3] for row in data])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        embedding = self.embeddings[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(embedding, dtype=torch.float32)\n",
        "\n",
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "        self.cumulative_sizes = [0]\n",
        "\n",
        "    def add_dataset(self, dataset):\n",
        "        self.data.extend([(dataset.images[i], dataset.embeddings[i]) for i in range(len(dataset))])\n",
        "        self.cumulative_sizes.append(len(self.data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, embedding = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(embedding, dtype=torch.float32)\n",
        "\n",
        "def create_combined_dataset(file_list, transform):\n",
        "    combined_dataset = CombinedDataset()\n",
        "    combined_dataset.transform = transform\n",
        "\n",
        "    for file in file_list:\n",
        "        data = np.load(file, allow_pickle=True)\n",
        "        dataset = SimpleDataset(data, transform)\n",
        "        combined_dataset.add_dataset(dataset)\n",
        "        del dataset, data  # Remove references to free up memory\n",
        "\n",
        "    return combined_dataset\n",
        "# Usage example:\n",
        "file_list = [\n",
        "    f\"{system_directory}/MetaData/gene_rgb_embedding.npy\",\n",
        "\n",
        "\n",
        "    # Add more dataset files as needed\n",
        "\n",
        "]\n",
        "\n",
        "dataset = create_combined_dataset(file_list, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3959ac-5934-48a7-b5eb-cc2b2afc62fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f3959ac-5934-48a7-b5eb-cc2b2afc62fd",
        "outputId": "300ad282-8735-4209-d504-39530a2e262e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 1536])\n",
            "tensor([-0.0304, -0.0100, -0.0075,  ..., -0.0166,  0.0087, -0.0098],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# load 8 sample embeddings from dataset\n",
        "sample_embeddings = torch.stack([dataset[i][1] for i in range(8)]).to(device)\n",
        "print(sample_embeddings.shape)\n",
        "print(sample_embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4239b4-9d19-41d0-b432-b4e347cc9f32",
      "metadata": {
        "id": "8d4239b4-9d19-41d0-b432-b4e347cc9f32"
      },
      "outputs": [],
      "source": [
        "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
        "def denoise_add_noise(x, t, pred_noise, z=None):\n",
        "    if z is None:\n",
        "        z = torch.randn_like(x)\n",
        "    noise = b_t.sqrt()[t] * z\n",
        "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
        "    return mean + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "580c7c17-92d2-4575-90f6-8a370aca896f",
      "metadata": {
        "id": "580c7c17-92d2-4575-90f6-8a370aca896f"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_ddpm_context(n_sample, context, save_rate=20):\n",
        "    # x_T ~ N(0, 1), sample initial noise\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    # array to keep track of generated steps for plotting\n",
        "    intermediate = []\n",
        "    for i in range(timesteps, 0, -1):\n",
        "        print(f'sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # reshape time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].repeat(n_sample, 1, 1, 1).to(device)\n",
        "\n",
        "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
        "        z = torch.randn_like(samples) if i > 1 else 0\n",
        "\n",
        "        # If using DataParallel, we need to handle the inputs differently\n",
        "        if isinstance(nn_model, nn.DataParallel):\n",
        "            # Prepare inputs\n",
        "            inputs = (samples, t, context)\n",
        "\n",
        "            # Run the model\n",
        "            eps = nn_model(*inputs)\n",
        "        else:\n",
        "            eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t, ctx)\n",
        "\n",
        "        samples = denoise_add_noise(samples, i, eps, z)\n",
        "        if i % save_rate==0 or i==timesteps or i<8:\n",
        "            intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd27fe11-ebc1-4715-a8b8-b333a6bb61d6",
      "metadata": {
        "id": "fd27fe11-ebc1-4715-a8b8-b333a6bb61d6"
      },
      "outputs": [],
      "source": [
        "# define sampling function for DDIM\n",
        "# removes the noise using ddim\n",
        "def denoise_ddim(x, t, t_prev, pred_noise):\n",
        "    ab = ab_t[t]\n",
        "    ab_prev = ab_t[t_prev]\n",
        "\n",
        "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
        "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
        "\n",
        "    return x0_pred + dir_xt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68b76b0-3ec8-49bf-80d7-5f63af56050f",
      "metadata": {
        "id": "e68b76b0-3ec8-49bf-80d7-5f63af56050f"
      },
      "outputs": [],
      "source": [
        "# fast sampling algorithm with context\n",
        "@torch.no_grad()\n",
        "def sample_ddim_context(n_sample, context, n=20):\n",
        "    # x_T ~ N(0, 1), sample initial noise\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    # array to keep track of generated steps for plotting\n",
        "    intermediate = []\n",
        "    step_size = timesteps // n\n",
        "    for i in range(timesteps, 0, -step_size):\n",
        "        print(f'sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # reshape time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
        "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
        "        intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfa96ab-44e8-4f71-807f-24b0e64fcb3b",
      "metadata": {
        "id": "3cfa96ab-44e8-4f71-807f-24b0e64fcb3b"
      },
      "outputs": [],
      "source": [
        "def visualize_embeddings_with_ep(sample_embeddings, ep):\n",
        "    nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_{ep}.pth\", map_location=device))\n",
        "    nn_model.eval()\n",
        "    plt.clf()\n",
        "    samples, intermediate = sample_ddpm_context(8, sample_embeddings)\n",
        "    # samples, intermediate = sample_ddim_context(8, sample_embeddings)\n",
        "    animation_ddpm_context = plot_sample(intermediate, 8, 2, save_dir, \"ani_run\", None, save=False)\n",
        "    display(HTML(animation_ddpm_context.to_jshtml()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oVRav8U1jZwT",
      "metadata": {
        "id": "oVRav8U1jZwT"
      },
      "outputs": [],
      "source": [
        "# helper function: perturbs an image to a specified noise level\n",
        "def perturb_input(x, t, noise):\n",
        "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda0f45e",
      "metadata": {
        "id": "bda0f45e"
      },
      "outputs": [],
      "source": [
        "# Training with context code\n",
        "nn_model.train()\n",
        "\n",
        "# Setup optimizer\n",
        "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)\n",
        "\n",
        "# Check if a checkpoint exists and load it if it does\n",
        "checkpoint_path = f\"{save_dir}checkpoint.pth\"\n",
        "start_epoch = 0\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "for ep in range(start_epoch, n_epoch):\n",
        "    print(f'epoch {ep}')\n",
        "\n",
        "    # linearly decay learning rate\n",
        "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
        "\n",
        "    pbar = tqdm(dataloader, mininterval=2)\n",
        "    epoch_loss = 0.0\n",
        "    for x, c in pbar:   # x: images  c: context\n",
        "        optim.zero_grad()\n",
        "        x = x.to(device)\n",
        "        c = c.to(x)\n",
        "\n",
        "        # Randomly mask out c\n",
        "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
        "        c = c * context_mask.unsqueeze(-1)\n",
        "\n",
        "        # Perturb data\n",
        "        noise = torch.randn_like(x)\n",
        "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device)\n",
        "        x_pert = perturb_input(x, t, noise)\n",
        "\n",
        "        # Use network to recover noise\n",
        "        pred_noise = nn_model(x_pert, t / timesteps, c=c)\n",
        "\n",
        "        # Loss is mean squared error between the predicted and true noise\n",
        "        loss = F.mse_loss(pred_noise, noise)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Compute the average loss for the epoch\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "    print(f'Average loss for epoch {ep}: {avg_epoch_loss}')\n",
        "\n",
        "    # Save model checkpoint\n",
        "    if ep % 1 == 0 or ep == int(n_epoch - 1):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "\n",
        "        # Update checkpoint\n",
        "        torch.save({\n",
        "            'epoch': ep,\n",
        "            'model_state_dict': nn_model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'loss': avg_epoch_loss,\n",
        "        }, checkpoint_path)\n",
        "        print('saved checkpoint at ' + checkpoint_path)\n",
        "\n",
        "        # Save model at ep\n",
        "        torch.save(nn_model.state_dict(), save_dir + f\"context_model_{ep}.pth\")\n",
        "        print('saved model at ' + save_dir + f\"context_model_{ep}.pth\")\n",
        "        # Plot samples using current saved model\n",
        "        visualize_embeddings_with_ep(sample_embeddings, ep)\n",
        "        nn_model.train()  # Reset to training mode after evaluation\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
